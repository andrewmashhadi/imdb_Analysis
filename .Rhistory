mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("randomForest")
tune_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(tune_spec)
# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(trees_train)
# choose 20 grid points automatically
tune_res <- tune_grid(
tune_wf,
resamples = trees_folds,
grid = 20
)
imdb_boxOffice <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_boxOffice.csv")
imdb_details <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details.csv")
imdb_top250_movies <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_movies.csv")
imdb_top250_shows <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_shows.csv")
imdb_details_extd <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details_extd.csv")
trees_df <- filter(imdb_details_extd, type == "Movie") %>%
na.omit()
trees_split <- initial_split(trees_df)
trees_train <- training(trees_split)
trees_test <- testing(trees_split)
# build recipe
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train,
)
# build model
tune_spec <- rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
tune_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(tune_spec)
# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(trees_train)
# choose 20 grid points automatically
tune_res <- tune_grid(
tune_wf,
resamples = trees_folds,
grid = 20
)
View(tune_wf)
View(tune_wf)
View(trees_train)
# build recipe
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train)
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
juiced
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres, threshold = 0.05)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
juiced %>% count(genres, sort = T)
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres, threshold = 0.2)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
juiced %>% count(genres, sort = T)
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres, threshold = 0.01)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
juiced %>% count(genres, sort = T)
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ . , data = trees_train) %>%
step_other(genres, threshold = 0.01) %>%
step_dummy(all_nominal(), -all_outcomes())
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
View(juiced)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres, threshold = 0.01)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
# build recipe (just instructions)
tree_rec <- recipe(imDbRating ~ budget+runtime+genres+year+rating, data = trees_train) %>%
step_other(genres, threshold = 0.01)
# prep actually uses the data
tree_prep <- prep(tree_rec)
juiced <- juice(tree_prep)
# build model
tune_spec <- rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
tune_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(tune_spec)
# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(trees_train)
trees_folds
# choose 20 grid points automatically
tune_res <- tune_grid(
tune_wf,
resamples = trees_folds,
grid = 20
)
View(tune_res)
# choose 20 grid points automatically
tune_res <- tune_grid(
tune_wf,
resamples = trees_folds,
grid = 2
)
View(tune_res)
tine_res
rf_grid <- grid_regular(
mtry(range = c(1, 10)),
min_n(range = c(1, 3)),
levels = 5
)
# choose 20 grid points automatically
tune_res <- tune_grid(
tune_wf,
resamples = trees_folds,
grid = rf_grid
)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("classification") %>%
set_engine("ranger")
tune_spec
tune_spec <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
tune_spec
seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
View(juiced)
View(juiced)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
library(vip)
install.packages("vip")
library(vip)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 2,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
library(vip)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 4,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
library(vip)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("randomForest")
library(vip)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
library(vip)
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
final_rf
View(final_rf)
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000
) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000,
min_n = 10
) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000,
min_n = 1
) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 2000,
) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
# seeing if importance plots still work with un-tuned mtry
final_rf <- rand_forest(
mtry = 3,
trees = 1000,
) %>%
set_mode("regression") %>%
set_engine("ranger")
final_rf %>%
set_engine("ranger", importance = "permutation") %>%
fit(imDbRating ~ .,
data = juice(tree_prep)
) %>%
vip(geom = "point")
final_wf <- workflow() %>%
add_recipe(tree_rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(trees_split)
final_res %>%
collect_metrics()
## using regression trees, random forest, or boosting, to predict imdbrating from
## budget, runtime, year, type, rating
## connect to db with imdb data
# library(RMySQL)
#
# drv <- dbDriver("MySQL")
#
# xdbsock <- ""
# xdbuser <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_USER")
# xpw     <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_PW")
# xdbname <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_DBNAME")
# xdbhost <- Sys.getenv("MAS405_AWS_MY_DB_ADMIN_HOST")
# xdbport <- as.integer( Sys.getenv("MAS405_AWS_MY_DB_ADMIN_PORT") )
#
# con <-
#   dbConnect(
#     drv,
#     user=xdbuser,
#     password=xpw,
#     dbname=xdbname,
#     host=xdbhost,
#     port=xdbport,
#     unix.sock=xdbsock
#   )
#
# dbListTables(con)
#
# dbGetInfo(con)
#
# imdb_boxOffice <- dbGetQuery(con, "SELECT * FROM imdb_boxOffice")
# imdb_details <- dbGetQuery(con, "SELECT * FROM imdb_details")
# imdb_details_extd <- dbGetQuery(con, "SELECT * FROM imdb_details_extd")
# imdb_top250_movies <- dbGetQuery(con, "SELECT * FROM imdb_top250_movies")
# imdb_top250_shows <- dbGetQuery(con, "SELECT * FROM imdb_top250_shows")
#
# dbDisconnect(con)
#
#
### store locally to save me $$$
#
# write.csv(imdb_boxOffice, "C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_boxOffice.csv")
# write.csv(imdb_details, "C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details.csv")
# write.csv(imdb_details_extd, "C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details_extd.csv")
# write.csv(imdb_top250_movies, "C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_movies.csv")
# write.csv(imdb_top250_shows, "C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_shows.csv")
#
#
imdb_boxOffice <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_boxOffice.csv")
imdb_details <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details.csv")
imdb_top250_movies <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_movies.csv")
imdb_top250_shows <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_top250_shows.csv")
imdb_details_extd <- read.csv("C:\\Users\\amiro\\Desktop\\Statistics 405\\Week 5\\Final_Project_Brainstorming\\imdb_details_extd.csv")
library(tidyverse)
########### USING PRUNED TREE #############
library(rpart)
library(partykit)
imdb_details_extd <- filter(imdb_details_extd, type == "Movie")
## print variables
names(imdb_details)
## simple single tree
tr <- rpart(imDbRating ~ budget+runtime+year+rating, data = imdb_details_extd)
## print plot to help choose cp
plotcp(tr)
## prune the tree
tr_2 <- prune(tr, cp = 0.016)
print(tr_2)
## plot the pruned tree
plot(as.party(tr_2), tp_args = list(id = FALSE))
## print plot to help choose cp
plotcp(tr)
new_extd_movies <- filter(imdb_details_extd, type == "Movie")
U_genres <- unique(unlist(sapply(distinct(imdb_details_extd, genres), strsplit, ", ")))
tr_df <- new_extd_movies[, c("title", "year", "runtime", "genres", "rating", "budget", "imDbRating")]
## make sure the genres are all in the same order
tt <-lapply(lapply(tr_df[, "genres"], strsplit, ", "), unlist)
tt <- lapply(tt, sort)
tt <- lapply(tt, paste, collapse=", ")
tr_df[, "genres"] <- unlist(tt)
## simple single tree
tr <- rpart(imDbRating ~ ., data = tr_df[, -c(1)])
## print plot to help choose cp
plotcp(tr)
## plot the pruned tree
plot(as.party(tr), tp_args = list(id = FALSE))
library(randomForest)
### COMPARING ERRORs
samp <- sample(5090)
rf_errors <- matrix(0, nrow=1, ncol=10)
pt_errors <- matrix(0, nrow=1, ncol=10)
for(k in 1:10){
from <- 1 + (k-1)*509
to <- 509*k # we will lose the last 8 observations
test <- na.omit(imdb_details_extd[samp[from:to],])
train <- imdb_details_extd[samp[-(from:to)],]
## rf
imdb_rf <- randomForest(imDbRating ~ budget+runtime+year+rating, data = imdb_details_extd, mtry = 3, na.action = na.omit)
## pruned tree
tr <- rpart(imDbRating ~ budget+runtime+year+rating, data = imdb_details_extd)
pt <- prune(tr, cp = 0.018)
## calc errors
rf_errors[k] <- mean((test$imDbRating - predict(imdb_rf, test))^2 )
pt_errors[k] <- mean((test$imDbRating - predict(pt, test))^2)
}
## compare errors
mean(rf_errors)
mean(pt_errors)
###### PREDICTION ACCRUACY #####
imdb_details_no_NA <- na.omit(imdb_details_extd)
imdb_rf = randomForest(imDbRating ~ budget+runtime+year+rating, data = imdb_details_no_NA, mtry = 3)
## compare the predictions to the data
tr_comp <- data.frame(imDbRating=imdb_details_no_NA$imDbRating, predictedRating=predict(imdb_rf, imdb_details_no_NA))
## approximate training accuracy
devs <- abs(tr_comp$imDbRating - tr_comp$predictedRating)
close_enoughs <- function(x) sum(devs <= x)/ length(devs)
x <- seq(from=0,to=1,by=0.01)
plot(x, sapply(x, close_enoughs), main="training accuracy", xlab="deviation tolerance", ylab="training data accuracy")
samp <- sample(5098)
train <- na.omit(imdb_details_extd[samp(1:4000), ])
test  <- na.omit(imdb_details_extd[samp(4001:5098), ])
imdb_rf = randomForest(imDbRating ~ budget+runtime+year+rating, data = train, mtry = 3)
## compare the predictions to the data
comp <- data.frame(imDbRating=test$imDbRating, predictedRating=predict(imdb_rf, test))
devs <- abs(comp$imDbRating - comp$predictedRating)
close_enoughs <- function(x) sum(devs <= x)/ length(devs)
x <- seq(from=0,to=1,by=0.01)
plot(x, sapply(x, close_enoughs), main="test accuracy", xlab="deviation tolerance", ylab="test data accuracy")
samp <- sample(5098)
train <- na.omit(imdb_details_extd[samp(1:4000), ])
test  <- na.omit(imdb_details_extd[samp(4001:5098), ])
imdb_rf = randomForest(imDbRating ~ budget+runtime+year+rating, data = train, mtry = 3)
## compare the predictions to the data
comp <- data.frame(imDbRating=test$imDbRating, predictedRating=predict(imdb_rf, test))
devs <- abs(comp$imDbRating - comp$predictedRating)
close_enoughs <- function(x) sum(devs <= x)/ length(devs)
x <- seq(from=0,to=1,by=0.01)
plot(x, sapply(x, close_enoughs), main="test accuracy", xlab="deviation tolerance", ylab="test data accuracy")
