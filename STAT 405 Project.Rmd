---
title: "R Notebook"
output:
  html_notebook: default
  pdf_document: default
---

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(RMySQL) -->

<!-- drv <- dbDriver("MySQL") -->

<!-- xdbsock <- "" -->

<!-- xdbuser <- Sys.getenv("MAS405_AWS_AM_DB_ROUSER_USER") -->
<!-- xpw     <- Sys.getenv("MAS405_AWS_AM_DB_ROUSER_PW") -->
<!-- xdbname <- Sys.getenv("MAS405_AWS_AM_DB_ROUSER_DBNAME") -->
<!-- xdbhost <- Sys.getenv("MAS405_AWS_AM_DB_ROUSER_HOST") -->
<!-- xdbport <- as.integer(Sys.getenv("MAS405_AWS_AM_DB_ROUSER_PORT")) -->

<!-- ### establish connection with Dave's DB -->

<!-- con <- dbConnect(drv, user=xdbuser, password=xpw, dbname=xdbname, host=xdbhost, port=xdbport, unix.sock=xdbsock) -->

<!-- ################## get info -->
<!-- dbListTables(con) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- imdb_boxOffice <- dbGetQuery(con, "SELECT * FROM imdb_boxOffice") -->
<!-- imdb_details <- dbGetQuery(con, "SELECT * FROM imdb_details") -->
<!-- imdb_top250_movies <- dbGetQuery(con, "SELECT * FROM imdb_top250_movies") -->
<!-- imdb_top250_shows <- dbGetQuery(con, "SELECT * FROM imdb_top250_shows") -->

<!-- dbDisconnect(con) -->

<!-- write.csv(imdb_boxOffice, "/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_boxOffice.csv") -->
<!-- write.csv(imdb_details, "/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_details.csv") -->
<!-- write.csv(imdb_top250_movies, "/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_movies.csv") -->
<!-- write.csv(imdb_top250_shows, "/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_shows.csv") -->
<!-- ``` -->


```{r}
library(tidyverse)
imdb_boxOffice <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_boxOffice.csv")
imdb_details <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_details.csv")
imdb_top250_movies <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_movies.csv")
imdb_top250_shows <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_shows.csv")
```


## Plot Summary - TF IDF and Most Common Words
```{r}
library(tm)
library(tidytext)

# Creating TF-IDF for the movie / show plots
tfidf <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, plot) %>%
  unnest_tokens(word, plot) %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  # count(fullTitle, word, sort = T) %>%
  anti_join(stop_words) %>%
  bind_tf_idf(word, fullTitle, n) %>%
  arrange(fullTitle, desc(n))

# Average TF-IDF score for each movie / show plot
temp <- tfidf %>%
  group_by(X, id, fullTitle, type, year, imDbRating) %>%
  summarise(avg_tf_idf = mean(tf_idf))

# Linear Regression - How well does plot TF-IDF predict rating
summary(lm(imDbRating ~ avg_tf_idf, temp))


# The 25 most frequent words in the movie plots and each word's average IMDB score
tfidf %>%
  filter(type == "Movie") %>%
  group_by(word) %>%
  summarise(n = n(),
            avg_rating = mean(imDbRating)) %>%
  slice_max(order_by = n, n = 25) %>%
  ggplot(aes(x = avg_rating, y = reorder(word, avg_rating), fill = avg_rating)) + geom_col(show.legend = F) + 
  labs(x = "Average Rating", y = "Word") + 
  ggtitle("25 Most Frequent Words in Movie Plots")

# The 25 most frequent words in the TV series plots and each word's average IMDB score
tfidf %>%
  filter(type == "TVSeries") %>%
  group_by(word) %>%
  summarise(n = n(),
            avg_rating = mean(imDbRating)) %>%
  slice_max(order_by = n, n = 25) %>%
  ggplot(aes(x = avg_rating, y = reorder(word, avg_rating), fill = avg_rating)) + geom_col(show.legend = F) + 
  labs(x = "Average Rating", y = "Word") + 
  ggtitle("25 Most Frequent Words in TV Series Plots")

# 10 most common words in movie / tv series plots, their frequency, and average rating
top10words <- tfidf %>%
  group_by(word) %>%
  summarise(n = n(),
            avg_rating = mean(imDbRating)) %>%
  slice_max(n, n = 10)

# Most common word and all plots where the word appeared
tfidf %>%
  filter(word %in% top10words$word[1]) %>%
  ggplot(aes(x = imDbRating, y = reorder(fullTitle, imDbRating), fill = word)) + 
  geom_col(show.legend = F)

```


## Genres and Relationship with IMDB Ratings
```{r}
# turning genres from genre column into dummy variables
temp <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, genres) %>%
  unnest_tokens(word, genres, token = str_split, pattern = ", ") %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  cast_sparse(fullTitle, word, n)

# combining imdb details data with dummy variables
data <- cbind(imdb_details %>% select(X, id, fullTitle, type, year, imDbRating, genres), data.frame(as.matrix(temp)))

## Linear Regression
# initial model with all genres
MASS::stepAIC(lm(imDbRating ~ comedy + drama + family + action + romance + adventure + sci.fi + biography + history + crime + mystery + thriller + fantasy + war + film.noir + western + musical + horror + music + animation + documentary + sport + game.show + reality.tv + news + short + talk.show, data), direction = "backward", trace = F)

# Final Regression Model after Backwards Selection - Note, this is the same as doing car::Anova
summary(lm(imDbRating ~ comedy + action + romance + adventure + sci.fi + biography + thriller + fantasy + animation + documentary, data))

## ANOVA
car::Anova(lm(imDbRating ~ comedy + action + romance + adventure + sci.fi + biography + thriller + fantasy + animation + documentary, data))
```


## Keywords and Relationship with IMDB Ratings
```{r}
# turning keywords / phrases from keywords column into dummy variables
temp <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, keywords) %>%
  unnest_tokens(word, keywords, token = str_split, pattern = ",") %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  cast_sparse(fullTitle, word, n)

# combining imdb details data with dummy variables
data <- cbind(imdb_details %>% select(imDbRating), data.frame(as.matrix(temp)))

# useless because there are 2114 keywords / phrases
summary(lm(imDbRating ~ ., data))

# going to use PCA to reduce dimensions, hopefully we can use PCs to describe relationship between keywords / phrases and imdb ratings
pc <- prcomp(data %>% select(-imDbRating))
variance_explained <- pc$sdev^2 / sum(pc$sdev^2)

qplot(c(1:4), variance_explained[1:4]) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)

# PCA did not help reduce dimensions, therefore, there probably is not a relationship between keywords / phrases and imdb score
```


## Clustering genres together into 2 groups - Not particularly interesting
```{r}
library(topicmodels)

# turning genres from genre column into dummy variables
temp <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, genres) %>%
  unnest_tokens(word, genres, token = str_split, pattern = ", ") %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  cast_sparse(fullTitle, word, n)

# beta is the probability the genre is generated from 1 of the 2 groups
library(reshape2)
lda <- tidy(LDA(temp, k = 2), matrix = "beta")

# getting the top 10 genres for each group 
top_genres <- lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_genres %>%
  mutate(genre = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, genre, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# lda %>%
#   mutate(topic = paste0("topic", term)) %>%
#   pivot_wider(names_from = topic, values_from = beta) %>% 
#   filter(topic1 > .001 | topic2 > .001) %>%
#   mutate(log_ratio = log2(topic2 / topic1)) %>%
#   ggplot(aes(x = log_ratio, y = reorder(genre, log_ratio))) + geom_bar(stat = "identity")
```


## Keyword - Genre Probabilities
```{r}
library(topicmodels)

# tgetting count of words from keyword column
word_counts <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, genres, keywords) %>%
  unnest_tokens(genre, genres, token = str_split, pattern = ", ") %>%
  filter(genre %in% c("comedy", "action", "romance", "sci-fi")) %>%
  unnest_tokens(word, keywords, token = str_split, pattern = ",") %>%
  anti_join(stop_words) %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

title_dtm <- word_counts %>%
  cast_dtm(fullTitle, word, n)

# beta is per topic per word probabilities (for each word, the probability of being in that genre)
library(reshape2)
lda <- tidy(LDA(title_dtm, k = 4), matrix = "beta")

# getting the top 5 terms for each group 
top_terms <- lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# The four genres vs the words with the highest probability of coming from that genre
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# gamma is per document per topic probabilities (the probability that the keywords from that movie/show are generated from that genre)
genre_gamma <- tidy(LDA(title_dtm, k = 4), matrix = "gamma")

movie_genre <- imdb_details %>% 
  select(fullTitle, genres) %>%
  unnest_tokens(genre, genres, token = str_split, pattern = ", ") %>%
  filter(genre %in% c("comedy", "action", "romance", "sci-fi"))

genre_gamma <- merge(movie_genre, genre_gamma, by.x = "fullTitle", by.y = "document", all = T)
genre_gamma <- genre_gamma[, c("genre", "fullTitle", "topic", "gamma")]

genre_gamma %>%
  arrange(topic) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ genre) +
  labs(x = "topic", y = expression(gamma))
```


## Plot - Genre Probabilities
```{r}
library(topicmodels)

# getting count of words from plot column
word_counts <- imdb_details %>%
  select(X, id, fullTitle, type, year, imDbRating, genres, plot) %>%
  unnest_tokens(genre, genres, token = str_split, pattern = ", ") %>%
  filter(genre %in% c("comedy", "action", "romance", "sci-fi")) %>%
  unnest_tokens(word, plot) %>%
  anti_join(stop_words) %>%
  group_by(X, id, fullTitle, type, year, imDbRating, word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

title_dtm <- word_counts %>%
  cast_dtm(fullTitle, word, n)

# beta is per topic per word probabilities (for each word, the probability of being in that genre)
library(reshape2)
lda <- tidy(LDA(title_dtm, k = 4), matrix = "beta")

# getting the top 5 terms for each group 
top_terms <- lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# The four genres vs the words with the highest probability of coming from that genre
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# gamma is per document per topic probabilities (the probability that the keywords from that movie/show are generated from that genre)
genre_gamma <- tidy(LDA(title_dtm, k = 4), matrix = "gamma")

movie_genre <- imdb_details %>% 
  select(fullTitle, genres) %>%
  unnest_tokens(genre, genres, token = str_split, pattern = ", ") %>%
  filter(genre %in% c("comedy", "action", "romance", "sci-fi"))

genre_gamma <- merge(movie_genre, genre_gamma, by.x = "fullTitle", by.y = "document", all = T)
genre_gamma <- genre_gamma[, c("genre", "fullTitle", "topic", "gamma")]

genre_gamma %>%
  arrange(topic) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ genre) +
  labs(x = "topic", y = expression(gamma))
```



## The Great Library Heist Example
I'm selecting 4 distinct movies / shows (2 movies, 2 shows) all with different genres and then, I'm going to apply the same methods above / in the example from the online textbook to see if we get similar results
```{r}
# Western Horror Comedy Drama; 2 movies and 2 shows
data <- imdb_details %>%
  filter(title %in% c("Once Upon a Time in the West", "It", "Parks and Recreation", "Mad Men"))

# Word count from all the plots after removing stop words
word_counts <- data %>%
  select(title, plot) %>%
  unnest_tokens(word, plot) %>%
  anti_join(stop_words) %>%
  count(title, word, sort = T)

# Document term matrix
data_dtm <- word_counts %>%
  cast_dtm(title, word, n)

data_lda <- LDA(data_dtm, k = 4)

# beta probabilities
data_topics <- tidy(data_lda, matrix = "beta")

# top terms for each group - there's a lot of ties because there's a lot of unique words
top_terms <- data_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

data_gamma <- tidy(data_lda, matrix = "gamma")
data_gamma

data_gamma %>%
  mutate(document = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma))
```


## Max Gamma Value for Each Movie / Show - still needs work
```{r}
# Word count from all the plots after removing stop words
word_counts <- imdb_details %>%
  select(title, plot) %>%
  unnest_tokens(word, plot) %>%
  anti_join(stop_words) %>%
  count(title, word, sort = T)

# Document term matrix
data_dtm <- word_counts %>%
  cast_dtm(title, word, n)

data_lda <- LDA(data_dtm, k = 4)

data_gamma <- tidy(data_lda, matrix = "gamma")

# getting each movie's max gamma so that each movie/show has one predicted topic / group number (1,2,3,4)
data_max_gamma <- data_gamma %>%
  group_by(document) %>%
  filter(gamma == max(gamma))

# all the movies/shows in each group 
data_max_gamma %>%
  ggplot(aes(topic, gamma, label = document)) +
  facet_wrap(~ topic) +
  geom_text() + xlim(-25, 25)

merge(data_max_gamma, word_counts, by.x = "document", by.y = "title", all.x = T) %>%
  select(-document) %>%
  group_by(topic) %>%
  slice_max(n, n = 5) %>%
  distinct(word, n) %>%
  mutate(word = reorder_within(word, n, topic)) %>%
  ggplot(aes(n, word, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

## KMeans Clustering
```{r}
library(factoextra)
library(cluster)

data <- imdb_details %>%
  select(fullTitle, year, runtime, imDbRating, budget, grossUSA, grossWorldwide, metacriticRating) %>%
  drop_na()

rownames(data) <- data$fullTitle
data <- data %>%
  select(-fullTitle) %>%
  scale()

fviz_nbclust(data, kmeans, method = "wss") # ideal number of clusters is 10

# calculate gap statistic based on number of clusters
gap_stat <- clusGap(data,
                    FUN = kmeans,
                    nstart = 25,
                    K.max = 10,
                    B = 50)

# plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)

km <- kmeans(data, centers = 10, nstart = 25)
fviz_cluster(km, data)
```


## K-Nearest Neighbor for Movie / Show Recommendation
```{r}
ten_nearest_neighbors <- function(data, fullTitleName) {
  temp <- data
  temp[, -1] <- scale(temp[, -1])
  X <- temp %>%
    filter(fullTitle == fullTitleName)
  
  distance <- sqrt((temp$year - X$year)^2 + (temp$runtime - X$runtime)^2 + (temp$imDbRating - X$imDbRating)^2 + (temp$budget - X$budget)^2 + (temp$grossUSA - X$grossUSA)^2 + (temp$grossWorldwide - temp$grossWorldwide)^2 + (temp$metacriticRating - X$metacriticRating)^2)
  
  data$distance <- distance
  
  return(data %>% arrange(distance) %>% head(10))
}

data <- imdb_details %>%
  select(fullTitle, year, runtime, imDbRating, budget, grossUSA, grossWorldwide, metacriticRating) %>%
  drop_na()


ten_nearest_neighbors(data, "Metropolis (1927)")
```


