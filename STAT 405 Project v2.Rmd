---
title: 'Unsupervised Learning: Movie/Show Recommendation'
author: "Ajay Patel"
output:
  pdf_document: default
  html_notebook: default
---

The ultimate goal for the unsupervised learning portion of the project is to make a movie/show recommendation system based on clustering and KNN. This means I did not use TF IDF or the Beta & Gamma probabilities (Topic Modeling) from the previous notebook. They are not particularly useful for our data. On this website, https://www.tidytextmining.com/topicmodeling.html, it states "topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for." In their examples, they used few documents (4 books) and lots of words (the words from all 4 books), making their clustering easy and more interpretable. In our case, we have lots of documents (movies / shows) and are limited to few words (plot summary), making it harder to extract meaning from the clusters or "topics."  

Here, I started with KModes clustering on the categorical features of the imdb_details dataset. The KModes cluster assignment was used as a feature in the KMeans clustering along with the other numerical features in the imdb_details datset. After I obtained the KMeans clustering cluster assignments, I included plots exploring the most common words from the plots per cluster, the most common keywords per cluster, the most common genres per cluster, and the highest rated movies/shows per cluster. 

The KMeans cluster assignment fed into the KNN along with the numerical features so that when given a title from the imdb_details dataset, the KNN function returns the 10 nearest neighbors from the same cluster. 

## Reading in the data
```{r}
library(tidyverse)
library(tm)
library(tidytext)
library(factoextra)
library(cluster)
# this package masks select function
library(klaR)
library(ggpubr)

# imdb_boxOffice <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_boxOffice.csv")
# imdb_details <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_details.csv")
# imdb_top250_movies <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_movies.csv")
# imdb_top250_shows <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_top250_shows.csv")
imdb_details_extd <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/imdb_details_extd.csv")
historic_cpi <- read.csv("/Users/ajaypatel21/Desktop/STAT405_Data/Project/historic_cpi.csv")

imdb_details_extd <- imdb_details_extd %>%
  filter(type == "Movie")

imdb_details_extd <- merge(imdb_details_extd, historic_cpi %>% dplyr::select(-X), by.x = "year", by.y = "Year", all.x = T)
```


## KModes Clustering
```{r}
kmodes_data <- imdb_details_extd
`%notin%` <- Negate(`%in%`)

# Cleaning language variable - some levels had small counts
kmodes_data$languages[kmodes_data$languages == "English, None"] <- "English"
kmodes_data$languages[kmodes_data$languages == "None, English"] <- "English"
kmodes_data$languages[kmodes_data$languages == "None, French"] <- "French"
kmodes_data$languages[grepl(",", kmodes_data$languages) == T] <- "Multilingual"
kmodes_data$languages[kmodes_data$languages == ""] <- "None"

kmodes_data$languages[kmodes_data$languages %notin% c("English")] <- "Multilingual/Foreign"

# Each show/movie has one genre of the 2+ genres listed, so I'm randomly picking 1 genre
set.seed(123)
genre <- c()
kmodes_data$genres[kmodes_data$genres == ""] <- "Other"
for (i in 1:5098) {
  num_genres <- length(strsplit(kmodes_data$genres, ", ")[[i]])
  random_genre_idx <- sample(1:num_genres, 1)
  genre[i] <- strsplit(kmodes_data$genres, ", ")[[i]][random_genre_idx]
}

# Cleaning genre - some genres had low counts
kmodes_data$genre <- genre
kmodes_data$genre[kmodes_data$genre %in% c("Animation", "Biography", "Documentary", "Family", "Fantasy", "Film-Noir", "History", "Music", "Musical", "Mystery", "News", "Other", "Reality-TV", "Sci-Fi", "Short", "Sport", "Talk-Show", "War", "Western")] <- "Other"

# Cleaning rating
kmodes_data$rating[kmodes_data$rating %notin% c("PG", "PG-13", "R")] <- "Other"

# KModes clustering
kmodes_data <- kmodes_data %>%
  dplyr::select(X, rating, genre, languages)

fviz_nbclust(kmodes_data, kmodes, method = "wss") # optimal number of clusters is 7
set.seed(123)
k.max <- 15
wss <- sapply(1:k.max, 
              function(k){sum(kmodes(kmodes_data[, 2:4], k, iter.max = 15)$withindiff^2)})

plot(1:k.max, wss,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of squares")

# Optimal number of clusters is 7
set.seed(123)
cl <- kmodes(kmodes_data[, 2:4], modes = 7)
kmodes_data$temporary_cluster <- cl$cluster # attaching the cluster number to the kmodes_data

# Manually releveling the cluster numbers using Hamming Distance
dissimilarity_score <- c(11, 12, 14, 12, 13, 12, 13)
cluster_number <- c(1,2,7,3,5,4,6)
cbind(cbind(cl$modes, dissimilarity_score), cluster_number) %>%
  arrange(dissimilarity_score)

# KModes cluster 6 --> 1, 5 --> 2, 1 --> 3, ...
kmodes_data$cluster <- 1 # initializing the new cluster column
kmodes_data$cluster[kmodes_data$temporary_cluster == 2] <- 2
kmodes_data$cluster[kmodes_data$temporary_cluster == 3] <- 7
kmodes_data$cluster[kmodes_data$temporary_cluster == 4] <- 3
kmodes_data$cluster[kmodes_data$temporary_cluster == 5] <- 5
kmodes_data$cluster[kmodes_data$temporary_cluster == 6] <- 4
kmodes_data$cluster[kmodes_data$temporary_cluster == 7] <- 6

kmodes_data <- kmodes_data %>%
  dplyr::select(-temporary_cluster)

# Plots
g1 <- ggplot(kmodes_data, aes(factor(cluster), fill = factor(genre))) + geom_bar(position = "fill") + xlab("KModes Cluster") + ylab("Proportion") + guides(fill = guide_legend(title = "Genre"))

g2 <- ggplot(kmodes_data, aes(factor(cluster), fill = factor(rating))) + geom_bar(position = "fill") + xlab("KModes Cluster") + ylab("Proportion") + guides(fill = guide_legend(title = "Rating"))

g3 <- ggplot(kmodes_data, aes(factor(cluster), fill = factor(languages))) + geom_bar(position = "fill") + xlab("KModes Cluster") + ylab("Proportion") + guides(fill = guide_legend(title = "Language"))

ggarrange(g1, g2, g3, ncol = 2, nrow = 2)
```


## Adjusting for Inflation
Note, I had code here to adjust grossUSA, grossWorldwide, and budget for inflation, but the adjusted variables made the clustering hard to interpret. 

## KMeans Clustering
```{r}
# Getting the numeric variables
temp <- imdb_details_extd %>%
  dplyr::select(X, fullTitle, year, runtime, imDbRating, budget, grossUSA, grossWorldwide, metacriticRating)

# Mergining the KModes cluster numbers with numeric data
kmeans_data <- merge(temp, kmodes_data, by = "X") %>%
  dplyr::select(-X)

# Filling NA's in each column with the column mean
for(i in 1:ncol(kmeans_data)) {
  kmeans_data[is.na(kmeans_data[,i]), i] <- mean(kmeans_data[,i], na.rm = TRUE)
}

# Dropping the categorical variables after the merge of temp and kmodes_data
rownames(kmeans_data) <- kmeans_data$fullTitle
kmeans_data <- kmeans_data %>%
  dplyr::select(-fullTitle, -rating, -genre, -languages) %>%
  scale()

# Assessing how many clusters we need
fviz_nbclust(kmeans_data, kmeans, method = "wss") # ideal number of clusters is 6

# Clustering algorithm and visualizing the clusters
set.seed(123)
km <- kmeans(kmeans_data, centers = 6)
fviz_cluster(km, kmeans_data, geom = "point", ggtheme = theme_minimal()) # default plot; will recreate below

# PCA is used for the x and y axis in plot above
pc <- prcomp(data.frame(kmeans_data))

# Saving the PCs and KMeans cluster numbers as dataframes so that I can join them together
df1 <- data.frame(pc$x)
df2 <- data.frame(km$cluster)

df1$movie <- rownames(df1)
df2$movie <- rownames(df2)

df <- merge(df1, df2, by = "movie") %>%
  mutate(km.cluster = factor(km.cluster)) %>%
  rename(Cluster = km.cluster)

# This finds the outermost points in each cluster to make the outline around the clusters
hull_df <- df %>%
  group_by(Cluster) %>%
  slice(chull(PC1, PC2))

# Recreated cluster plot from above
ggplot(df, aes(x = PC1, y = PC2, color = Cluster, label = movie)) + 
  geom_text(check_overlap = T, size = 3) + 
  theme_minimal() +
  geom_polygon(data = hull_df, alpha = 0, linetype = 2, size = 0.25)
```


## Plots about the Clusters
```{r}
# Attaching the cluster number to original data
cluster <- data.frame(km$cluster)
cluster$fullTitle <- rownames(cluster)

movie_info <- merge(imdb_details_extd, cluster, by = "fullTitle") %>%
  dplyr::select(X, id, title, type, year, imDbRating, genres, plot, keywords, km.cluster)

# Most common words from plot per cluster
movie_info %>%
  unnest_tokens(word, plot) %>%
  anti_join(stop_words) %>%
  count(km.cluster, word, sort = T) %>%
  group_by(km.cluster) %>%
  slice_max(order_by = n, n = 4) %>%
  mutate(word = reorder_within(word, n, km.cluster)) %>%
  ggplot(aes(n, word, fill = factor(km.cluster))) + 
  geom_col(show.legend = F) + 
  facet_wrap(~ km.cluster, scales = "free_y") + 
  scale_y_reordered() +
  ggtitle("Most Common Words From Plots Per Cluster")

# Most common genres per cluster
movie_info %>%
  unnest_tokens(word, genres, token = str_split, pattern = ", ") %>%
  anti_join(stop_words) %>%
  count(km.cluster, word, sort = T) %>%
  group_by(km.cluster) %>%
  slice_max(order_by = n, n = 4) %>%
  mutate(word = reorder_within(word, n, km.cluster)) %>%
  ggplot(aes(n, word, fill = factor(km.cluster))) + 
  geom_col(show.legend = F) + 
  facet_wrap(~ km.cluster, scales = "free_y") + 
  scale_y_reordered() +
  ylab("genre") +
  ggtitle("Most Common Genres Per Cluster")

# Most common keywords per cluster
movie_info %>%
  unnest_tokens(word, keywords, token = str_split, pattern = ",") %>%
  anti_join(stop_words) %>%
  count(km.cluster, word, sort = T) %>%
  group_by(km.cluster) %>%
  slice_max(order_by = n, n = 3) %>%
  mutate(word = reorder_within(word, n, km.cluster)) %>%
  ggplot(aes(n, word, fill = factor(km.cluster))) + 
  geom_col(show.legend = F) + 
  facet_wrap(~ km.cluster, scales = "free_y") + 
  scale_y_reordered() +
  ylab("keywords") +
  ggtitle("Most Common Keywords Per Cluster")

# Top 4 highest rated movies/shows per cluster
movie_info %>%
  group_by(km.cluster) %>%
  slice_max(order_by = imDbRating, n = 4) %>%
  mutate(title = reorder_within(title, imDbRating, km.cluster)) %>%
  ggplot(aes(x = imDbRating, y = title, fill = factor(km.cluster))) + 
  geom_col(show.legend = F) +
  facet_wrap(~ km.cluster, scales = "free_y") + 
  scale_y_reordered() +
  ggtitle("Highest Rated Movies & Shows Per Cluster")
```


## K-Nearest Neighbor for Movie / Show Recommendation
```{r}
ten_nearest_neighbors <- function(data, fullTitleName) {
  temp <- data
  
  # getting the cluster the movie/show is in 
  cluster <- (temp %>%
    filter(fullTitle == fullTitleName))$km.cluster
  
  temp[, -1] <- scale(temp[, -1])
  X <- temp %>%
    filter(fullTitle == fullTitleName)
  
  # euclidean distance for KNN
  distance <- sqrt((temp$year - X$year)^2 + (temp$runtime - X$runtime)^2 + (temp$imDbRating - X$imDbRating)^2 + (temp$budget - X$budget)^2 + (temp$grossUSA - X$grossUSA)^2 + (temp$grossWorldwide - temp$grossWorldwide)^2 + (temp$metacriticRating - X$metacriticRating)^2)
  
  data$distance <- distance
  
  # constrains the recommendations to be in the same cluster
  return(data %>% arrange(distance) %>% head(10))
}


knn_data <- merge(imdb_details_extd, cluster, by = "fullTitle") %>%
  dplyr::select(fullTitle, year, runtime, imDbRating, budget, grossUSA, grossWorldwide, metacriticRating, km.cluster)

# Filling NA's in each column with the column mean
for(i in 1:ncol(knn_data)) {
  knn_data[is.na(knn_data[,i]), i] <- mean(knn_data[,i], na.rm = TRUE)
}

ten_nearest_neighbors(knn_data, "Avengers: Endgame (2019)")
```



